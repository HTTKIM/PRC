import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from concurrent.futures import ThreadPoolExecutor
import numpy as np
import time
import sys
from dotenv import load_dotenv
load_dotenv()

# ë²¡í„° DB + ì„ë² ë”©
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
vectorstore = FAISS.load_local(
    folder_path=r"C:\\python-proc\\1_DATA_Embedding\\FAISS_store_0414",
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)

retriever_sim = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})
retriever_mmr = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 6, "fetch_k": 20, "lambda_mult": 1.0})

# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
def cosine_sim(a, b):
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)

def is_semantically_similar(term1, term2, threshold=0.6):
    try:
        vec1 = embedding_model.embed_query(term1)
        vec2 = embedding_model.embed_query(term2)
        return cosine_sim(vec1, vec2) >= threshold
    except:
        return False

# GPT í˜¸ì¶œ
def stream_gpt_response(prompt):
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, streaming=True)
    full_text = ""
    for chunk in llm.stream(prompt):
        content = chunk.content or ""
        full_text += content
        yield content
    return full_text

# Streamlit UI
st.set_page_config(layout="wide")
st.title("ğŸ§ª GPT ê¸°ë°˜ í‘œì¤€ ê³µì • ìƒì„±ê¸°")

with st.form("query_form"):
    col1, col2 = st.columns(2)
    with col1:
        ì›ë£Œ = st.text_input("ì›ë£Œ", value="ì›ì§€")
    with col2:
        ìƒì‚°í’ˆ = st.text_input("ìƒì‚°í’ˆ", value="ì‹ ë¬¸")

    submitted = st.form_submit_button("ê³µì • ìƒì„±í•˜ê¸°")

if submitted:
    query = f"ì›ë£Œ: {ì›ë£Œ} / ìƒì‚°í’ˆ: {ìƒì‚°í’ˆ}"

    # ê²€ìƒ‰
    with st.spinner("ğŸ” ìœ ì‚¬ ê³µì • ê²€ìƒ‰ ì¤‘..."):
        with ThreadPoolExecutor() as executor:
            sim_docs = executor.submit(retriever_sim.invoke, query).result()
            mmr_docs = executor.submit(retriever_mmr.invoke, query).result()
        all_docs = {doc.page_content: doc for doc in sim_docs + mmr_docs}.values()

    # ì˜ë¯¸ ìœ ì‚¬ë„ í•„í„°ë§
    ìœ ì‚¬ê³µì • = []
    for doc in all_docs:
        if is_semantically_similar(ì›ë£Œ, doc.metadata.get("ì›ë£Œ", "")) and \
           is_semantically_similar(ìƒì‚°í’ˆ, doc.metadata.get("ìƒì‚°í’ˆ", "")):
            ìœ ì‚¬ê³µì •.append(doc.metadata.get("ê³µì •", ""))

    if not ìœ ì‚¬ê³µì •:
        st.warning("â— ìœ ì‚¬í•œ ê³µì •ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ GPT ìƒì„±ì´ ìƒëµë©ë‹ˆë‹¤.")
        st.write("í‘œì¤€ ê³µì •ì„ ë„ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        ì˜ˆì‹œë“¤ = "\n".join([f"- {flow}" for flow in ìœ ì‚¬ê³µì •])
        system_prompt = (
            "ë„ˆëŠ” ê³µì¥ì„ ì ê²€í•˜ê³  í•´ë‹¹ ì‚¬ì—…ì¥ì˜ ê³µì •ì„ ë‹¨ìœ„ê³µì • ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì•¼. "
            "ë‹¤ìŒ ì˜ˆì‹œëŠ” ì‹¤ì œ ìœ ì‚¬í•œ ì‚¬ë¡€ë“¤ì´ê³ , ì´ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ íë¦„ì„ ì°¸ê³ í•˜ì—¬ ê³µì •ì„ ë§Œë“¤ì–´ì•¼ í•´. "
            "ê±°ì§“ë§ì€ í—ˆìš©ë˜ì§€ ì•Šìœ¼ë©°, ì˜ˆì‹œì™€ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ë‚´ë©´ ì•ˆ ë¼. "
            "ë‹¨ìœ„ê³µì • ëª…ì¹­ì€ ì•„ë˜ ìœ ì‚¬ê³µì •ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©. "
            "ë‹µë³€ì€ ë‹¨ìœ„ê³µì •ì´ í™”ì‚´í‘œë¡œ ì—°ê²°ëœ íë¦„ì˜ í˜•íƒœë¡œ íšŒì‹ í•´ì¤˜."
        )
        final_prompt = f"{system_prompt}\n\nì°¸ê³  ê³µì • ì˜ˆì‹œ:\n{ì˜ˆì‹œë“¤}\n\ní‘œì¤€ ê³µì • ì‘ì„± ëŒ€ìƒ:\n{query}"

        st.subheader("GPTê°€ ì‘ì„±í•œ í‘œì¤€ ê³µì •")

        with st.spinner("ë‹µë³€ ì‘ì„±ì¤‘..."):
            response_box = st.empty()
            stream_text = ""

            for chunk in stream_gpt_response(final_prompt):
                stream_text += chunk
                formatted_text = stream_text.replace("â†’", " â†’ ")
                response_box.markdown(
                    f"""
                    <div style='font-size:20px; line-height:1.8; font-family: "Noto Sans KR", sans-serif;'>
                        {formatted_text}
                    </div>
                    """,
                    unsafe_allow_html=True
                )

        # ìœ ì‚¬ë„ í‰ê°€
        gpt_vector = embedding_model.embed_query(stream_text)
        best_score = 0
        best_match = ""
        for ref in ìœ ì‚¬ê³µì •:
            ref_vector = embedding_model.embed_query(ref)
            score = cosine_sim(gpt_vector, ref_vector)
            if score > best_score:
                best_score = score
                best_match = ref

        with st.expander("ğŸ“ ê°€ì¥ ìœ ì‚¬í•œ ì°¸ê³  ê³µì •"):
            st.markdown(f"**ê³µì •:** {best_match}")
            st.markdown(f"**ìœ ì‚¬ë„ ì ìˆ˜:** {round(best_score, 4)}")
